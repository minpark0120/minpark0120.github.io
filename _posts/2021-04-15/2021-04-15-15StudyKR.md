---
title: 2021-04-15-Thu-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------- -----------------------------

# RL
## Policy Gradient Alogirhtm
```
정책을 직접적으로 모델링하고 최적화
    특정 parameter로 구성된 함수, 
```

### On-Policy Policy Gradient
```
최적화해서 구하기를 원하는 target policy와 매우 유사한 policy로부터 training sample이 수집되어야 한다 
```

### Off-Policy Policy Gradient
```
Full Trajectory가 필요하지 않기 때문에, Sample efficiency 측면에서 지난 episode에서 trajectory를 뽑아 재사용한다.
Sample을 수집하는데 사용된 behavior policy는 "known policy"인데, 이를 B(a|s)라고 한다.
    Objective Function은 behavior policy에 의해서 정의된 state distrubution에서 계산한 reward를 모두 더한 값이다.
```
## 가치 기반 에이전트
```
Neural Net을 이용하여 가치 기반 에이전트를 학습
    1. 정책이 고정되어 있을 때, Neural Net을 이용해 정책의 가치 함수 Vpi(s) 학습            

* Neural Net을 학습하려면 예측과 정답 사이 차이를 뜻하는 손실 함수를 정의해야한다.
    
실제 가치 함수를 대체하는 MC Return과 TD Target이 있다.
```
## 딥 Q러닝
```
내재된 정책
1. Q Learning
    greedy와 e-greedy를 사용해 액션을 선택하고 관측함, 만족할 떄까지 반복

2. Experience Replay and Target Network
    DQN: Deep Q-Network 
        성능을 끌어올리기 위해 사용됨
        겪은 경험을 재사용해 더 좋은 강화학습을 진해함
        재사용을 위해 replay buffer 사용

        별도의 target network를 두어 정답을 안정적으로 변하게함
        네트워크를 두개를 준비해, 정답지를 계산할 때 사용하는 네트워크를 얼려두어
            업데이트가 많이 되면 최신 파라미터로 교체함.
```

