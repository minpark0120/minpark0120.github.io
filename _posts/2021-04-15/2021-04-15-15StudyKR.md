---
title: 2021-04-15-Thu-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------- -----------------------------

# RL
## Policy Gradient Alogirhtm
```
정책을 직접적으로 모델링하고 최적화
    특정 parameter로 구성된 함수, 
```

### On-Policy Policy Gradient
```
최적화해서 구하기를 원하는 target policy와 매우 유사한 policy로부터 training sample이 수집되어야 한다 
```

### Off-Policy Policy Gradient
```
Full Trajectory가 필요하지 않기 때문에, Sample efficiency 측면에서 지난 episode에서 trajectory를 뽑아 재사용한다.
Sample을 수집하는데 사용된 behavior policy는 "known policy"인데, 이를 B(a|s)라고 한다.
    Objective Function은 behavior policy에 의해서 정의된 state distrubution에서 계산한 reward를 모두 더한 값이다.
```
## 가치 기반 에이전트
```
Neural Net을 이용하여 가치 기반 에이전트를 학습
    1. 정책이 고정되어 있을 때, Neural Net을 이용해 정책의 가치 함수 Vpi(s) 학습            

* Neural Net을 학습하려면 예측과 정답 사이 차이를 뜻하는 손실 함수를 정의해야한다.
```