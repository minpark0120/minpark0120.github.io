---
title: 2021-05-18-Tue-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------


# RL
## DDPG
```
1.	Problem
DQN이 high-dimension observation space 문제를 해결했지만, discrete하거나 low-dimension action space에 국한된다는 문제점이 있었다.

2.	Technical approaches
DDPG는 Deep Q-Learning을 actor-critic을 활용해 deterministic policy를  학습하고 효과를 continuous action로 확장하였다. 

3.	Major contribution
DDPG는 높은 차원이면서 continuous한 action space의 policy를 학습할 수 있다. Replay buffer를 사용하여 data를 random하게 뽑아 correlation을 minimize하였다. Target Q-network를 생성해 학습 도중에 origin Q-network parameter를 가능한 고정시켜 학습이 converge할 수 있게 한다.


4.	Main performance evaluation result(s)
Cartpole이나 Torc라는 continuous 환경을 적용하여 low-dimension과 high-dimension에서 전부 잘 됨을 확인하였다. Target Network를 적용할 시 학습 수렴이 빨라지는 것을 확인하였다.

5.	Stochastic Critics and Comments 
Model-free 기반이기 때문에 학습 시간이 오래 걸린다. Continuous action space에 강화학습 알고리즘을 적용하였고 성능이 뛰어나기 때문에 자주 쓰인다.

```

## D4PG

```

1.	Problem

DQN은 discrete action space에서 finite 하다는 문제점이 있다.  DDPG는Continuous control 문제가 있다.
2.	Technical approaches

DDPG에서 continuous control 문제점을 해결하기 위해 distributional하도록 개선점을 적용하였다. D4PG는 DDPG Off-policy하고 actor-critic 방법이다. 
	
3.	Major contribution

DPG에서 나온policy gradient update 방법을 expectation of the Q-value distribution으로 바꾼다. 미래의 step에 얻어질 보상을 반영하기 위해 한번의 step이 아닌 N번의 step TD Target error를 구한다.  또한 K개의 actor를 운영해, 평행하게 경험을 쌓고, replay buffer에 data를 넣는다. 그리고 replay buffer에서 sampling을 수행해 prioritized experienced replay를 가질 수 있다.

4.	Main performance evaluation result(s)

모든 실험에서 D4PG의 성능이 제일 좋다는 것을 확인하였다. 이 때, unroll length에 따라 D3PG, D4PG가 unstable 할 수 있다. Distributional critic update를 포함하며 hardest task였던 humanoid 와 acrobat에 도움이 되었다.

5.	Stochastic Critics and Comments 

 Performance 능력을 향상할 수 있지만, unstable한 update로 lead 할 수도 있다. D4PG는 한 때, 최상의 모델 프리 에이전트였다.
 

```

## DDPG class
```
DQN이 continous domain에서 straight forward하지 못하다.
    only handle discrete and low-dimensional action space
    find action that maximizes the action-value function

DDPG is based on DPG with DQN

function approximation is in a stable and robust way with two innovations
    network is trained off-policy with samples from a replay buffer
    trained with a target Q network to give consistent target during temporal difference backups+
```

## D4PG class
```
distributed distributional deterministic policy gradient
정리하자면 하나의 critic을 고른다기 보다 분포를 만들어 고루고루 고를 수 있도록 하는것이다.
또한 1 step 말고 n step을 거치도록하여 학습하게 하였다.
priority를 집어 넣어 explore 할 수 있도록 하였다.

distributional update의 포함
multiple distributed workers의 사용
n-step return의 사용

하지만 less crucial of priority to the overall D4PG
could lead to unstable updates
```