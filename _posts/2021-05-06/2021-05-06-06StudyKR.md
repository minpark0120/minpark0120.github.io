---
title: 2021-05-06-Thu-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------

# Deep Q Learning
## RL에 DL 방법론
```
-Raw pixel을 받아와 directly input data로 다룬 것
-CNN을 function approximator로 이용한 것
-하나의 agent가 여러 종류의 atari game을 학습할 수 있는 능력을 갖춘 것
-Experience replay를 사용하여 data efficiency를 향상한 것

```

## Experience Replay
```
이전까진 환경과 상호작용하며 얻은 on-policy sample을 통해 파라미터를 업데이트하였지만, 이 방법은 sample에 대한 의존도가 커서
정책이 수렴하지 못하고 Oscilliate 할 수 있다.

이 문제를 해결하고자 Experience Replay 아이디어가 나오게 되었다.
Time-Step별로 얻은 샘플들을 tuple 형태로 data set에 저장해두고, randomly draw하여 mini-batch를 구성하고 update 하는 방법이다.

무한으로 data set에 저장할 수 없기 때문에, N으로 고정하고, FIFO 형태로 저장한다

Input으로써 arbitrary length를 받는데 어려움이 있기 때문에 function을 정의해 pre-processing하여 state의 length를 고정시킨다.

Data sample을 한번 update하고 버리지 않고, random sampling을 함으로 데이터를 유용하게 사용할 수 있다.

RL에서 발생할 수 있는 state간 높은 correlation을 줄일 수 있다. 현재 업데이트된 파라미터를 통해 다음 training 대상이 되는 sample을 어느정도 결정할 수 있다.

현재 policy에 따라 argmax 액션을 고르고, 그 다음 training sample은 이 action에 따라 결정된다. . 
이를 current sample에 의해 다음 training sample이 dominate되었다고 말합니다. 
본 논문에 따르면, 이렇게 되면 불필요한 feedback loops를 돌거나, parameter들이 local minimum에 빠지거나 diverge하는 경우를 찾아낼 수 있어서,이를 방지할 수 있다고 합니다. 
주의할 점은, experience replay를 사용할 땐, 반드시 off-policy로 learning해야하는데, 이는 current parameter가 update하는 sample들과 다르기때문입니다. 

```

## DQN Algorithm
![DQN](https://user-images.githubusercontent.com/59559270/117253636-647fa300-ae82-11eb-9d3a-3412686ccb05.png)
```
유한한 용량 N을 가진 replay 메모리 D를 정의하고 Q-Learning을 위해 Q-Value를 초기화한다.
하나의 에피소드 동안 sequence를 초기화시키고 pre-process 시킨다.
    주어진 히스토리 중에 down-sizing과 gray scale을 취하고 GPU 환경에 맞게 square로 crop한 뒤, last 4 frame을 stack으로 쌓는다
    Time-Step마다 e-greedy 방식으로 action을 취하고, 이를 통해 reward와 다음 state를 pre-processing하여 experience을 구성하고 D에 저장한다.
    D에 저장된 sample을 mini-batch로 취하여 사전에 정의된 loss를 최소화하여 GD 업데이트를 한다. 
    DQN Network는 CNN을 이용한다.
```
![cnn](https://user-images.githubusercontent.com/59559270/117253644-65b0d000-ae82-11eb-9d0f-95cd8d669411.png)

### DQN Example
https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html

```py
import gym #환경을 위한 변수 
import math 
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from itertools import count
from PIL import Image

import torch
import torch.nn as nn ## 신경망
import torch.optim as optim ## 최적화
import torch.nn.functional as F
import torchvision.transforms as T##시각 태스크를 위한 유틸리티


env = gym.make('CartPole-v0').unwrapped

# matplotlib 설정
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()

# GPU를 사용할 경우
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

### Environment 설명
https://engineering-ladder.tistory.com/61   