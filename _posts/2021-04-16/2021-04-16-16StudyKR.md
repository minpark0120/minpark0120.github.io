---
title: 2021-04-16-Fri-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------

# RL

## DQN
```
Experience replay와 frozen target network을 통해서 Q-function에 대한 학습을 안정화 시킨다.
    이산적인 공간에서 동작한다.
```

## DPG
```
Policy는 deterministic decision을 하도록 모델링한다. 
    policy가 single action을 요구할 때, policy function의 gradient를 계산한다.

```

## DDPG
```
DPG에 DQN을 결합시킨 model-free off-policy actor-critic 알고리즘이다.
    DQN이 원래는 이산적인 공간에서 동작하지만, DDPG에서는 actor-critic framework을 활용하여
        deterministic policy를 활용하며, 효과를 continuous space까지 확장되었다.

좀 더 나은 exploration을 위해 Noise를 추가할 수 있다.

Soft update를 actor network과 critic network으로 각각 적용된다.
```

## DP4G
```
DDPG에 distrubutional 개념이 반영되도록 몇가지 개선점을 적용한다.
1. Distrubtional Critic
2. N-step returns
3. Multiple Distributed Parallel Actors
4. Prioritized Experienced Replay
```

## MADDPG
```
여러개의 agent가 local information만 가지고, task를 처리할 수 있도록 
    서로 협력하는 환경으로 DDPG를 확장시킴.
```