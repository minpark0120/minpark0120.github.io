---
title: 2021-04-08-Thu-Study-KR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------

# C

## GEO
```
```

# RL

## MDP를 알때의 방법론
```
 1. 작은문제
 2. MDP를 알 때 = 보상 함수와 전이 확률 행렬을 안다. (즉, 상태 S에서 액션 A를 실행하면 다음 상태가 어떻게 정해지는지, 보상이 어떻게 될지를 미리 알고 있다.)
    플래닝: 정책을 알 때 이를 이용해 정책을 개선해 나가는 과정
        PREDICTION 문제: 정책이 주어졌을 때, 각 상태의 밸류를 평가하는 문제
        CONTROL 문제: 최적의 정책함수를 찾는 문제
*테이블 기반 방법론: 주로 작은 문제이기 때문에 가능
```

### 밸류 평가: 반복적 정책 평가
```
테이블의 값들을 초기화한 후, 벨만 기대방정식을 반복적으로 사용하여 테이블에 적어 놓은 값들을 조금씩 업데이트해 나가는 방법론
    보상 함수와 전이 확률을 알고 있다. (보상 = -1 고정, 감마 = 1)
1. 테이블 초기화
2. 한 상태의 값을 업데이트
    벨만 기대 방정식: 현태 상태 S의 가치를 다음에 도달하게 되는 상태S'에 가치로 표현한 식
    종료 상태의 경우 가치가 '0'
3. 모든 상태에 대해 2의 과정을 적용
4. 2~3 과정 반복: 결국 실제 값에 수렴
```

### 최고의 정책 찾기: Policy iteration
```
정책 평가와 정책 개선을 번갈아 수행하며 정책이 수렴할 때까지 반복하는 방법론
1. 임의의 정책으로 초기화
2. 정책 평가: 반복적 정책 평가, 이 단계에서는 고정된 정책에 대해 각 상태의 밸류를 구한다.
    정책을 따랐을 때, 각 상태의 가치 평가이기 때문에, 정책 평가라고 부른다. 한번만 평가해도 optimal이 된다.
3. 정책 개선: greedy 정책 생성
4. 2~3 반복
5. 수렴하는 곳이 바로 최적 정책, 최적 가치가 된다.
```

### 최고의 정책 찾기 - Value iteration
```
오로지 최적 정책이 만들어내는 최적 value 하나만 바라봄
테이블 기반 방법론
MDP를 모두 아는 상황에서 최적 밸류를 알면 최적 정책을 곧바로 얻을 수 있다.