---
title: 2021-04-08-Thu-Study
categories: [study]
comments: true
---
-------------------------------------------------------------------------------

# C

## GEO
```
```

# RL

## Methodology when knowing MDP
```
 1. A small problem
 2. Knowing the MDP = Knowing the compensation function and the transition probability matrix. 
    (In other words, if you execute action A in state S, you know in advance how the next state will be determined and what will be the reward.)
    Planning: When you know a policy, the process of improving it by using it
        PREDICTION Problem: The problem of evaluating the value of each state given a policy.
        CONTROL Problem: Finding the Optimal Policy Function
*Table-based methodology: mainly because it is a small problem
```

### Value evaluation: iterative policy evaluation
```
After initializing the values ​​of the table, a methodology that gradually updates the values ​​
    written in the table by repeatedly using the Bellman expectation equation
    You know the reward function and the probability of the transition. (Reward = -1 fixed, gamma = 1)

1. Table initialization
2. Update the value of one state
    Bellman Expectation Equation: An equation that expresses the value of the current state S as a value to the next state S'
    In the case of exit status, the value is '0'
3. Apply the process of 2 for all states
4. Repeat the process 2~3: eventually converge to the actual value
```

### Finding the best policy: Policy iteration
```
A methodology that alternates between policy evaluation and policy improvement and repeats until policy convergence
1. Initialize with a random policy
2. Policy evaluation: Iterative policy evaluation, in this step, the value of each state is calculated for a fixed policy.
    When a policy is followed, it is called a policy evaluation 
        because it is the valuation of each state. Even if you evaluate it once, it becomes optimal.
3. Policy improvement: greedy policy creation
4. Repeat 2~3
5. The place of convergence becomes the optimum policy and optimum value.
```

### Finding the best policy-Value iteration
```
Only looking at the optimal value created by the optimal policy
Table-based methodology
If you know all the MDP and know the optimal value, you can get the optimal policy right away.
```



