---
title: 2021-04-16-Fri-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------

# RL

## DQN
```
Experience replay와 frozen target network을 통해서 Q-function에 대한 학습을 안정화 시킨다.
    이산적인 공간에서 동작한다.
```

## DPG
```
Policy는 deterministic decision을 하도록 모델링한다. 
    policy가 single action을 요구할 때, policy function의 gradient를 계산한다.

```

## DDPG
```
DPG에 DQN을 결합시킨 model-free off-policy actor-critic 알고리즘이다.
    DQN이 원래는 이산적인 공간에서 동작하지만, DDPG에서는 actor-critic framework을 활용하여
        deterministic policy를 활용하며, 효과를 continuous space까지 확장되었다.

좀 더 나은 exploration을 위해 Noise를 추가할 수 있다.

Soft update를 actor network과 critic network으로 각각 적용된다.
```

## DP4G
```
DDPG에 distrubutional 개념이 반영되도록 몇가지 개선점을 적용한다.
1. Distrubtional Critic
2. N-step returns
3. Multiple Distributed Parallel Actors
4. Prioritized Experienced Replay
```

## MADDPG
```
여러개의 agent가 local information만 가지고, task를 처리할 수 있도록 
    서로 협력하는 환경으로 DDPG를 확장시킴.
```
## Reinforce Algirutgm
```
정책으로 에피소드 하나에 해당하는 데이터를 얻고, 
    해당 데이터로 파라미터를 업데이트하고,
        업데이트된 정책을 이용해 또 다음 에피소드의 경험을 얻고
            또 그 데이터로 강화하는 반복작업.

*파라미터가 변하지 않거나 서능 개선이 일어나지 않을 때까지 진행
```

## Actor-Critic
### Q Actor Critic
```
정책 네트워크와 밸류 네트워크를 함꼐 학습하는 actor-critic 방법론
    critic은 현재 정책 함수의 가치를 학습하는 방향으로 업데이트
    actor는 Q의 평가를 통해 결과를 보고 강화 혹은 약화
```

### Advantage Actor-Critic
```
Q Actor Critic에 비해 advantage Actor-Critic은 gradient 추정치의 변동성을
    줄여즘으로써 학습 효율에 도움이 된다.

하지만, 3쌍의 Neural Net을 필요로한다.
```

### TD Actor Critic
```
Advantage Actor Critic보다 한쌍 덜 필요하다(Q)
```

