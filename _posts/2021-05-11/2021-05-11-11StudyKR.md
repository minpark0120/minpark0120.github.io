---
title: 2021-05-11-Tue-StudyKR
categories: [studyKR]
comments: true
---
-------------------------------------------------------------------------------



# A3C 논문
## Asynchronous Methods for Deep Reinforcement Learning
```
1.	Problem
이전까지의 DL을 적용한 RL 알고리즘은 간단한 온라인 RL의 combination도 unstable한 문제점이 있었다. 이를 해결하기 위해 여러 아이디어를 제안하였는데, 해당 아이디어들은 non-stationary하고 online RL updates가 강하게 correlated된 단점이 있었다. 이를 해결하기 위해 Experience replay에 기반한 DeepRL 알고리즘을 적용하였고 다른 time-step에서 random sample해서 stationarity를 줄이고 업데이트를 decorrelate 시켰으나, 이 방법은 더 많은 메모리와 계산양을 요구했다. 게다가 오래된 정책으로 발생된 데이터 업데이트를 위해 off-policy 알고리즘만을 요구하였다. 

2.	Technical approaches
이 논문에서는 experience replay 대신, 여러 환경에서 평행하게 다수의 에이전트를 비동기적으로 학습하는 것을 제안한다. 다수의 에이전트가 평행하게 학습하기 때문에 다양한 state를 경험하게 되고 에이전트 데이터가 stationary process로 decorrelate된다.

위 아이디어는 여러 강화학습 알고리즘을 적용할 수 있고 다른 GPU로 병렬 처리를 하는 알고리즘보다 single machine with a standard multi-core CPU로 실험하는 것이 더 좋은 결과를 선보였다.

3.	Major Contribution
Actor-Critic의 on-policy policy와 Q-Learning의 off-policy policy의 아이디어를 사용하고 비동기적 actor-critic learner를 이용하지만 단일 머신의 다중 CPU 쓰레드를 사용한다. 다중 actor-learner를 사용함으로써 서로 다른 탐험 정책을 사용하여 온라인 업데이트와 비교했을 때 덜 correlated가 된다.  병렬된 actor-learner로 인해 학습 시간을 줄일 수 있다. 또한 리플레이 메모리를 사용하지 않아도 되기 때문에, 메모리를 아낄 수 있다. 
4.	Main performance evaluation result(s)
Atari
Atari에 적용하였을 때 DQN보다 빨리 학습하는 평균을 보임
A3C가 다른 방법들보다 퍼포먼스가 좋음
   TORCS Car Racing 
	다른 에이전트보다 A3C가 성능이 제일 좋았음
   Continuous Action Control Using the MuJoCo Physics Simulator
	A3C 알고리즘 결과 값이 유의미하게 나왔음
   
5.	Critics and Comments 
하나의 에이전트 대신 여러 개의 에이전트를 쓴다는 간단한 아이디어지만 엄청나게 좋은 결과를 가져왔고, RL의 베이스가 되었다. 굳이 단점을 찾자면, 여러 개의 쓰레드가 동시에 작동하기 때문에, 에이전트의 속도가 느리다는 문제점이 있다. 이 논문에서 더 발전한 논문인 DDPG,TRPO,PPO,등을 추후 더 읽어볼 예정이다.
```

# DPG 논문
## Deterministic Policy Gradient Algorithms
```
1.	Problem
연속적인 액션 Space에서 RL은 Stochastic Policy Gradient의 성능이 Deterministic Policy Gradient보다 떨어진다. SPG는 state와 action모두에 대해 평균을 취하지만 DPG는 state에 대해서만 평균을 취하고 결과적으로 action space의 차원이 커질수록 data efficiency가 DPG가 더 좋아지게 된다.

2.	Technical approaches
Stochastic Actor-Critic 알고리즘은 Actor와 Critic이 번갈아 가면서 동작하며 stochastic policy를 최적화한다. Actor는 policy gradient를 올라가는 방향으로 policy 파라미터를 업데이트함으로써 stochastic policy를 발전시키고 Critic은 SARSA, Q-learning같은 TD Learning을 이용해 action-value function의 parameter를 업데이트 함으로써 Qw(s,a)가 Qπ(s,a) 과 유사해지도록 한다.  Compatible condition에 부합하는 Qw(s,a)를 사용하게 되면 bias또한 발생하지 않는다. Reinforce나 actor-critic method는 on-policy gradient method를 사용하는데, 최적화 하기 위해서는 target policy와 유사한 policy로부터 학습 샘플이 필요하고 off-policy가 이를 할 수 있게 해준다. 
3.	Major contribution
현재의 Stochastic policy가 deterministic policy와 variation variable에 의해서 다시 파라미터화 되는 경우 stochastic policy는 б=0일 때 결과적으로 deterministic policy와 동일하다. Deterministic policy와 비교했을 때, stochastic policy가 전체 sate와 action space에 대해서 데이터를 완성해야 해 더 많은 샘플들이 필요하다. On-policy actor-critic이 매번 수행될 때, 두개의 action이 deterministic하게 선택되며 SARSA는 아래에서 계산될 새로운 gradient에 의해 on-policy parameter를 업데이트한다. 하지만 환경내에 충분한 noise가 있지 않는 한 충분한 exploration을 보장하기 어렵고, 따라서 policy에 noise를 더하거나 다른 stochastic behavior policy를 수행해 off policy처럼 학습하는 방법을 취한다.
DPG 알고리즘은 파라미터화된 actor function을 지녀 state에서 특정한 action을 지정함으로써 현재 policy를 지정하고 critic function을 통해 상태와 action의 가치를 벨만 방정식을 사용해 학습하여 평가하며 actor function은 policy gradient 방식으로 학습된다. 이 때 action function은 objective function을 gradient assent로 최대화시키고 그 때의 policy parameter를 찾는 것이 학습의 목적이다.

4.	Main performance evaluation result(s)
Stochastic Actor-Critic과 COPDAC간 성능 비교를 수행했을 때, action dimension이 커질수록 성능 차이가 심했고, DPG의 data efficiency가 더 좋았지만 time-step이 증가할수록 성능 차이가 줄어 드는 것을 확인하였다. COPDAC-Q, SAC, OFFPAC-TD를 비교하였을 때 COPDA-Q가 성능이 약간 더 좋았고 학습이 더 빠르게 이루어졌다. 
Octopus Arm 실험에서는 20개의 action dimension들과 50개의 state dimension들을 컨트롤하여 target을 맞추는 실험이었는데, COPDAC-Q 사용시 dimension이 큰 octopus arm을 잘 control하여 target을 잘 맞췄다. 
5.	Critics and Comments 
고전적인 actor-critic 방식에 neural function approximator를 결합하는 방식으로는 어려운 문제에 대해 불안정함을 가지고 있는 문제점이 있다. 이를 발전시킨 논문이 DDPG 이다.
```
